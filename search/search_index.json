{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the AI Training Dataset Sustainability Toolkit","text":""},{"location":"#guidance-to-the-toolkit","title":"Guidance to the Toolkit","text":"<p>The purpose of this toolkit is to support Lacuna Fund grantees and the wider machine learning community in publishing sustainable AI training datasets in standard formats. The toolkit is designed as a step-by-step playbook, with guidance and checklists that researchers can reference as they prepare their datasets to ensure widespread reuse by others in the Machine Learning (ML) community.  The toolkit covers topics including guidance on producing high-quality datasets before publication, selection of appropriate platforms to host the dataset and, activities they can undertake to promote the datasets and maximize their discoverability and impact. The toolkit also  emphasizes ethical considerations including guidelines on how to protect personal information and privacy. Examples provided within this initial draft of the toolkit are specific to datasets for use in ML in Agriculture, Climate, Natural Language Processing, Health and Energy. </p> <p>This work was carried out by the Local Development Research Institute (LDRI) with support from Lacuna Fund and Canada\u2019s International Development Research Centre (IDRC).</p> <p>We welcome feedback on these guidelines by contacting Leonida at leo@developlocal.org  or Cecilia at cecilia@developlocal.org.</p>"},{"location":"TL%3BDR/","title":"TL;DR","text":""},{"location":"TL%3BDR/#checklist-for-dataset-sustainability","title":"Checklist for dataset sustainability","text":"<ol> <li>Produce High-Quality Datasets<ul> <li> Clean and validate the data</li> <li> Use established or standardized naming conventions for variables</li> <li> Publish in standard, machine-readable formats</li> <li> Ensure that annotated data has consistent and accurate labels</li> <li> Test the data on various models</li> </ul> </li> <li>Create Privacy-Preserving Datasets<ul> <li> Adhere to local regulations and protocols regarding data protection and privacy</li> </ul> </li> <li>Document your Dataset Extensively<ul> <li> Create a data sheet</li> <li> Document any models used to augment or annotate the data</li> <li> Version the dataset</li> </ul> </li> <li>Host datasets accessibly<ul> <li> Publish in widely acceptable and available data-sharing platforms</li> <li> License the data with appropriate open-source licenses</li> <li> Compress the data</li> </ul> </li> <li>Promote your datasets widely and Build Community for Your Dataset<ul> <li> Host competitions to innovate/problem-solve with the dataset. </li> <li> Publish the dataset in a data journal, conference or paper</li> <li> Generate DOIs for your dataset</li> <li> Social media and online forums </li> <li> Dataset registries and catalogues:</li> <li> Partner with academic institutions and industry players</li> <li> Organize regular meetups and webinars </li> </ul> </li> <li>Create and Maintain your datasets ethically</li> <li>Consider sustainable funding for your Dataset<ul> <li> Grants</li> <li> Partnerships</li> <li> Crowd-sourcing</li> </ul> </li> </ol>"},{"location":"beforepublish/","title":"Produce High-Quality Datasets","text":""},{"location":"beforepublish/#1-clean-and-validate-the-data","title":"1. Clean and validate the data","text":"<p>Cleaning and validating AI training data is the first important step in ensuring that the data is accurate, reliable, and useful for machine learning. One of the main benefits of cleaning and validating data is the ability to reduce errors and biases in AI models. If training data contains inaccuracies, incomplete or irrelevant information, or outliers, it can lead to errors in the model development process and the resultant AI applications built on the datasets.</p> <p>This process includes identifying and removing duplicates, outliers, inaccuracies, and irrelevant data. In this way, researchers can improve the quality and effectiveness of their AI models, ensuring that they are more accurate and reliable. </p> <p>Suggested steps to clean and validate your datasets include:</p> <ol> <li>De-duplication: Remove duplicate records to ensure each entity or occurence is represented only once.</li> <li>Handling Missing Data: Use imputation techniques or interpolation to fill in missing values. Alternatively, completely remove records with excessive missing data.</li> <li>Standardization: Convert your data or variables within the dataset to a common format or unit for consistency.</li> <li>Removing Noise: Filter out irrelevant data points that do not contribute to meaningful analysis.</li> <li>Outlier Detection: Identify and correct or remove outliers that may be due to data entry errors or device malfunctions (in the case of images, these could be photos with occlusion).</li> <li>Cross-Validation: Validate data against other reliable sources or benchmark datasets to ensure accuracy and consistency.</li> <li>Consistency Checks: Ensuring data consistency across different fields, records, and over time.</li> <li>Expert Review: Co-opt domain experts to review the data for accuracy, relevancy, and plausibility.</li> <li>Ground-Truthing: Validate data against ground-truth data collected through field surveys or manual annotations.</li> <li>Temporal and Spatial Consistency: If relevant, check for consistency of values over time and across different spatial locations.  11 Image Quality: As needed, ensure sufficient overlap between consecutive images (e.g. in the case of satellite imagery) to enable accurate stitching and depth perception while also ensuring that the area of interest is completely.</li> <li>Model Validation: Validating data by running models and comparing the output with observed or historical data.</li> </ol>"},{"location":"beforepublish/#2-use-established-or-standardized-naming-conventions-for-variables","title":"2. Use established or standardized naming conventions for variables","text":"<p>Use standard vocabularies and ontologies to describe the data and its attributes. This promotes consistency and interoperability across different datasets and makes it easier to combine and analyze data from multiple sources, as variables with the same name have the same meaning and interpretation. Using descriptive names makes the dataset contents and tasks clear without needing to consult external documentation. Research similar datasets published and where appropriate use similar naming conventions for your variables. Ensuring that training datasets are interoperable is essential for enabling data sharing and reuse across different platforms and applications. Interoperable datasets can be easily integrated with other datasets, making it possible to perform cross-dataset analysis and training of AI models. </p> NLPHealthClimate/Agriculture/Energy <p>Some common variable naming conventions for NLP datasets include: </p> <ul> <li>text - Raw input text</li> <li>context - Surrounding text </li> <li>document - Long documents</li> <li>question - Question text</li> <li>answer - Answer text</li> <li>label - Classification labels  </li> <li>sentiment - Sentiment polarity labels </li> <li>entities - Named entities</li> <li>relations - Relations</li> <li>id - Unique identifier</li> <li>topic - Document topic</li> <li>annotations - Additional annotations</li> </ul> <p>Typically health datasets, especially image-based datasets produced for AI tend to be multidimensional e.g. imaging datasets like MRI and X-ray that include 2D and 3D images,time-series data, and multi-parametric data that combine different types of health measurements. The effectiveness of clinical data standards is determined by the semantic interoperability.</p> <p>Some common variable naming conventions for these datasets include: </p> <ul> <li>Study and Series Identifiers: Each imaging study and series within it typically has a unique identifier.</li> <li>Patient Identifiers: Patient-specific information like ID, age, sex, and sometimes anonymized names.</li> <li>Date and Time Stamps: Indicating when the study was conducted.</li> <li>Modality: Type of imaging (e.g., MRI, CT, X-ray).</li> <li>Body Part Examined: The anatomical region imaged.</li> <li>View Position: For radiographs, the view or angle of the image (e.g., AP for anterior-posterior).</li> <li>Imaging Parameters: Such as slice thickness, echo time, repetition time in MRI.</li> <li>Conventional Disease Names: Derived from the International Classification of Diseases(ICD)</li> </ul> <p>Additionally you may want to consider using a file naming conventions such as a combination of patient identifiers, study date, modality, and other relevant metadata for interoperability, ease of sorting and retreiving data files.</p> <p>Datasets produced for climate, energy, and agriculture contexts, despite their distinct applications, share several commonalities (therefore, examples are grouped for these domains): </p> <ul> <li> <p>They have both temporal and spatial dimensions: i.e. over time and over various locations e.g. climate datasets include weather patterns over time, energy datasets may track consumption or production across different regions over periods, and agriculture datasets can show crop yields or soil conditions across various locations and seasons.</p> </li> <li> <p>They are multivariate in nature: For climate, this could be temperature, humidity, precipitation, and wind speed; for energy, it might include consumption levels, production sources, and energy prices; in agriculture, variables could include crop type, yield, soil quality, and weather conditions.</p> </li> <li> <p>They have common ontologies for weather, agriculture, ecology which provides semantic interoperability across datasets.</p> </li> </ul>"},{"location":"beforepublish/#3-publish-in-standard-machine-readable-formats","title":"3. Publish in standard, machine-readable formats","text":"<p>Using a standard, machine-readable format for AI training datasets is crucial for ensuring that the data is easy to use in different contexts and is readily consumed in machine learning pipelines. Machine-readable formats allow data to be read, interpreted, and processed by computers without additional pre-processing by most ML platforms. Using a standardized format also reduces the risk of errors and inconsistencies in data interpretation. Most hosting platforms and ML communities have already established acceptable formats in which the datasets can be published.</p> <p>Example</p> NLPHealthClimate/Agriculture/Energy <p>Acceptable formats for NLP-based data include JSON, CSV, TSV or txt. Each format has its own strengths and is chosen based on the specific requirements of the NLP task e.g.</p> <ul> <li> <p>For simple sequence, labelling/classification plain text or CSV works.</p> <ul> <li>Plain text - Each sample is one sentence or document in a text file, one per line. </li> <li>CSV - Comma or tab-separated values file with one or more text columns for the sample and label columns. </li> </ul> </li> <li> <p>For complex tasks requiring hierarchies with multiple levels of details, such as annotated corpora with metadata, etc. JSON or XML works better.</p> <ul> <li>JSON - Each sample is a JSON object with text and labels as attributes.</li> <li>XML - Samples and labels wrapped in XML tags</li> </ul> </li> <li> <p>For sequence labelling tasks such as PoS and named entity recognition, CoNLL format works best</p> <ul> <li>CoNLL - Tab-separated format that includes tokenization with labels for each token.</li> </ul> </li> </ul> <p>DICOM (Digital Imaging and Communications in Medicine) is the most widely used standard for storing and transmitting medical imaging information and related data. DICOM standardizes the formats for medical images, the protocols for imaging techniques (such as MRI and X-rays), and other vital data related to medical imaging.</p> <p>For associated non-imaging data (like clinical data), formats like CSV, JSON, or XML might be used, with standardized field names and structures.</p> <p>Examples of naming conventions and schemas to observe include:</p> <ol> <li>Descriptive Names that include Time Frames: Use descriptive names that clearly indicate the dataset's content. For example, <code>GlobalSurfaceTemperature_1980_2020</code>.</li> <li>Standard Abbreviations: Utilize standard abbreviations (e.g., <code>temp</code> for temperature, <code>prod</code> for production) and indicate units of measurement e.g. Celsius (<code>temp_C</code>) or energy in megawatts (<code>energy_MW</code>).</li> <li>** Spatial Resolution**: If applicable, include spatial resolution, e.g., <code>AirQuality_10kmGrid</code>.</li> <li>Standardized  Names for variables, e.g., using <code>date</code> for dates, <code>latitude</code> and <code>longitude</code> for geographical coordinates.</li> </ol>"},{"location":"beforepublish/#4-ensure-that-annotated-data-has-consistent-and-accurate-labels","title":"4. Ensure that annotated data has consistent and accurate labels","text":"<p>This is important in creating AI models trained on the dataset that are reliable and trustworthy. Use industry standards for annotating data where available, such as those established by the CoNLL-2003 dataset for named entity recognition tasks. These standards ensure that labels are consistent and accurate across different datasets and models.</p> <p>Establish annotation guidelines that define the criteria for labelling data. These guidelines should include specific instructions for annotators, such as examples of correctly labelled data and rules for handling ambiguous cases. </p> <p>Validate the labels by comparing the annotations of different annotators or by using a gold standard dataset. This ensures that the labels are consistent and accurate across different annotators and that the annotations meet the required standards.</p> <p>Tip</p> <p>You can use inter-annotator agreement (IAA) measurements to test that different annotators largely agree on the assigned labels</p>"},{"location":"beforepublish/#5-test-the-data-on-various-models","title":"5. Test the data on various models","text":"<p>Testing AI training data on various models is essential for ensuring that the data is robust and effective for machine learning purposes. By testing data on multiple models, researchers can identify any limitations in the data and improve its quality using the steps above. Additionally, testing on various models can ensure that the data is not biased towards specific models, ensuring that the resulting AI models are reliable and accurate. Where necessary, testing with models also enables you to correctly identify and declare biases in the dataset. Additionally, by testing the training data on multiple models, researchers can ensure that their work is reproducible and promotes transparency and accountability to other machine learning practitioners who will use the training datasets.</p> <p>Example</p> <p>Test Models can be hosted as notebooks, e.g Jupyter Notebooks, or on Github when publishing your datasets.</p> <p>This paper on the Kencorpus dataset  provides an example of how test models were as proof of concepts on a Question-Answering subset of the dataset.</p>"},{"location":"definition/","title":"Sustainable AI Training Datasets","text":""},{"location":"definition/#defining-sustainability","title":"Defining Sustainability","text":"<p>We define sustainable AI training datasets as high-quality curated datasets that are created with long-term accessibility and broad usability in mind, prioritise societal benefit, respect and reflect the diversity of the communities they represent and are created with an awareness of the specific context and through non-exploitative practices.</p>"},{"location":"definition/#why-dataset-sustainability-is-important","title":"Why Dataset Sustainability is Important","text":"<p>Publishing and maintaining sustainable AI training datasets is critical to developing and advancing machine learning and artificial intelligence technologies. The datasets produced through the Lacuna Fund grant aim to serve as the foundation for training models and algorithms in low-resourced contexts. The quality and relevance of the training data are therefore critical factors in determining the effectiveness and accuracy of the resulting models and their eventual impact in these contexts. Therefore, publishing high-quality and sustainable datasets is paramount for ensuring that AI systems are developed ethically and responsibly.</p> <p>One of the main benefits of publishing AI training datasets is increasing the availability of knowledge resources and promoting collaboration in building context-relevant models and AIs. Researchers have the opportunity to also learn from each other's work and build on previous discoveries. This is also advantageous as it conserves resources and ensures communities do not suffer from research fatigue as datasets are reused. </p> <p>As AI systems become more complex and are integrated into various sectors and applications that impact societies it is essential to ensure that these systems are developed responsibly and that their decisions are explainable and understandable. By publishing training datasets, researchers  help promote transparency and accountability by providing information about the data used to train the models. Consequently, these open datasets create incentives for researchers to create high-quality models.</p> <p>Sustainable AI training datasets are also critical for ensuring that AI technologies developed have a positive impact on society. By ensuring that datasets are representative and inclusive of diverse populations, researchers can help to minimize potential biases and ensure that AI systems are fair and equitable.</p> <p>Ultimately, by publishing sustainable training datasets, researchers can ensure that their expected benefits are realized and maintained even after the end of the Lacuna Fund grant.</p>"},{"location":"discoverability/","title":"Promoting Discoverability and Reuse","text":"<p>Once published in accessible and findable formats, it is important to actively promote your datasets to increase their visibility, discoverability, and impact within the AI research community.</p>"},{"location":"discoverability/#build-community-for-your-dataset","title":"Build Community for Your Dataset","text":"<p>In the context of ML training datasets, we refer to the community as two groups of stakeholders. </p> <p>The first of these are the individuals and communities who are represented in or affected by the dataset. Engagement with data subjects ensures that the dataset is not only comprehensive with declared biases but is respectful and considerate of the nuances of the represented languages, indigenous knowledge and cultures. This engagement to produce culturally sensitive datasets enhances their relevance and applicability in context-aware AI applications. </p> <p>The second group of stakeholders include communities of researchers and developers with specific domain expertise who can provide valuable insights into the terminology and use case development and data gaps unique to their contexts. </p> <p>Both sets of communities can provide feedback that iteratively improves the quality of the dataset and ensures the dataset remains relevant and robust.</p> <p>Tip</p> <p>This engagement can be in the form of consultations with native speakers, local linguists, language experts, and community members. This collaborative effort also creates a sense of ownership and representation among the communities involved.</p> <p>Collaborations with initiatives like Common Voice and Masakhane can also broaden the dataset's scope and enhance its quality. These partnerships contribute significantly to creating a dataset that is not only linguistically rich but also domain-relevant.</p> <p>The following are suggested strategies to engage the community in dataset development and improvement. Active engagement involves creating an ecosystem where stakeholders, including researchers, developers, end-users, and subject matter experts, actively participate and contribute. </p> <ul> <li>Hosting competitions to innovate/problem-solve with the dataset. </li> </ul> <p>While these competitions encourage data scientists and researchers to learn and apply their skills to real-world problems, they are also an effective way for dataset creators to increase the visibility and re-use of the dataset in new applications, promote specific use cases and receive feedback and validation on the quality of the datasets from other researchers in the community.  Examples of popular competition platforms include Kaggle and Zindi, and for academic researchers CodaLab. Incentives may sometimes be required to increase the level of engagement and participation by community researchers.</p> <ul> <li>Publishing the dataset in a data journal, conference or paper </li> </ul> <p>Additionally, promoting datasets through academic conferences, workshops, and webinars can help reach a more targeted audience of researchers and practitioners in the field. Presenting datasets and their potential applications at these events can spark interest, encourage collaboration, and facilitate knowledge exchange among experts. </p> <p>Consider writing and publishing a paper detailing your dataset, its development process, and its performance in test models. This can be submitted to conferences, journals, or even as a preprint on platforms like arXiv, Data in Brief or Journal of Open Research Software. These publications focus on the datasets themselves and can help increase their visibility within the research community.</p> <ul> <li>Data citation and DOIs </li> </ul> <p>By publishing on sites that allow indexing of datasets e.g. using URI and DOIs, datasets are discoverable from research papers and on web pages that can be easily indexed and ranked by search engines. Additionally, use appropriate keywords in metadata and datasheets for better visibility in search engines.</p> <ul> <li>Social media and online forums </li> </ul> <p>Share and promote datasets through your social media platforms, mailing lists, and relevant online forums and discussion boards such as AI/ML subreddits or AI research groups on LinkedIn. These provide opportunities for community members can discuss dataset development, share ideas, and provide feedback.</p> <ul> <li>Dataset registries and catalogues</li> </ul> <p>Contribute to existing dataset registries and catalogues, such as Registry of Open Data on AWS or DataHub, which can increase the visibility of the datasets and make them more discoverable.</p> <ul> <li>Partnerships with academic institutions and industry</li> </ul> <p>Collaborate with academic institutions and industry partners to create, maintain, and share datasets, which can increase their visibility and discoverability among researchers in the field. These institutions can create research projects, internships, or thesis work that contributes to the dataset.</p> <ul> <li>Organize regular meetups and webinars </li> </ul> <p>These topical meetups provide you with an opportunity to discuss progress, challenges, and future directions of the dataset. Inviting expert speakers and conducting trainings using your dataset in these workshops will educate and engage your data science community.</p>"},{"location":"documentation/","title":"Document your Datasets Extensively","text":"<p>Maintaining detailed documentation, including data provenance, annotation guidelines, and any preprocessing steps shows transparency about data sourcing, annotation methodologies and the intended use of the dataset. The following are suggested methods of dataset documentation:</p>"},{"location":"documentation/#1-create-a-data-sheet","title":"1. Create a data sheet","text":"<p>Creating a data fact sheet is an important step in ensuring that AI training datasets are transparent and trustworthy. The data sheet can include the following components:</p> <ul> <li>A comprehensive description of the data itself: This should include descriptions of what fields/variables the dataset contains.</li> <li>A description of the provenance of the data: includes information about the source of the data, sampling methodology, data collection instruments and protocols, and any relevant information about the data collection process. </li> <li>A description of the data cleaning and preprocessing steps e.g. if duplicates were removed, outliers were corrected, or missing values were imputed, provide details about these steps and their rationale.</li> <li>Information about any annotations or labels: This includes information about any annotations or labels that were applied to the data, including information about the labelling process, any standards or guidelines used, and the accuracy or reliability of the labels.</li> <li>Relevant metadata: This includes catalogue information about the dataset\u2019s origin, purpose, time of creation, creator, formats, file size, and any other relevant standards used that can help users understand and work with the data. This should also include the version of the dataset if various iterations of the dataset are to be published.</li> <li>Contact information for the data owner in case users need clarification or further information about the data.</li> </ul> <p>Tip</p> DatasheetsMetadata <p>Datasheets are useful tools for documenting the datasets used for training and evaluating machine learning models. Datasheets contain questions about dataset motivation, composition, collection, pre-processing, labelling, intended uses, distribution, and maintenance. The following is a good template for dataset documentation and an example in use: </p> <p>Datasheet for datasets</p> <p>Datasheets: Multimodal datasets for Bemba Language</p> <p>Healthsheet: Development of a Transparency Artifact for Health Datasets</p> <p>Documenting metadata in standardised metadata formats and schemas is crucial for interoperability. The Dublin Core is a good standard to use. It consists of a set of vocabulary terms used to describe web resources such as video, images, web pages, and, importantly, datasets. It includes 15 core elements like title, creator, subject, description, and more. Applying Dublin Core standards to your dataset ensures that the basic and essential metadata is captured in a standardized way.</p> <p>It is important to document a dataset's compliance with relevant regulations (e.g Local Data Protection Laws, HIPAA or GDPR) and ethical guidelines, including any consent forms or IRB approvals. Due to how the data was collected, it may be necessary to provide additional information on terms of data use, specifically restrictions of use.</p>"},{"location":"documentation/#2-document-any-models-used-to-augment-or-annotate-the-data","title":"2. Document any models used to augment or annotate the data","text":"<p>If using a model to augment or annotate the data, it is important to use explainable methods and document the intended use cases, the data, model inputs, and the accuracy of the model to make it easy to reproduce the results. </p> <p>Document the original data and inputs used to train the model, including any preprocessing steps or transformations that were performed. Include documentation of any parameters or hyperparameters that were set. This helps to ensure that this augmentation process can be easily reproduced. It is also important to include the accuracy of the model and provide information about any metrics or evaluation techniques used to measure model performance. </p> <p>Document the code for the test models as well as all the necessary dependencies for the code to run to make it easier for other researchers to replicate your setup. Sharing the models on Jupyter notebooks or on Github can help ensure your models are reproducible.</p>"},{"location":"documentation/#3-version-the-dataset","title":"3. Version the dataset","text":"<p>Versioning the AI training dataset enables other researchers to track changes to the dataset, roll back to previous versions if necessary, and maintain consistency and reliability in the data used for analysis. Using a version control system such as Git enables you to track each new change or update to the dataset. Each update to the dataset should include a description of the changes made, such as adding new data, removing duplicates, or modifying annotations. If you have variations to your datasets for different purposes, it is important to also tag each version of the dataset e.g. if used for specific experiments or publications. This makes it easy to identify specific versions of the dataset that were used for different purposes.</p>"},{"location":"ethicalAI/","title":"Ethical Considerations","text":""},{"location":"ethicalAI/#ethical-considerations-for-responsible-ai","title":"Ethical Considerations for Responsible AI","text":"<p>We suggest through out your dataset creation process to keep the following important considerations in mind. </p> <ul> <li>Intellectual Property Rights: </li> </ul> <p>Respect copyrights and intellectual property laws when using or sharing data.</p> <ul> <li>Informed Consent: </li> </ul> <p>Ensure that data subjects are aware of how their data will be used and have consented to it, especially for sensitive data.</p> <ul> <li>Ethical Guidelines and Training: </li> </ul> <p>Develop and enforce ethical guidelines internally and train data scientists and annotators contributing to the creation of datasets on these principles.</p> <ul> <li>Continuous Monitoring: </li> </ul> <p>Regularly monitor and reassess the datasets published for any arising unintended impacts or ethical concerns on the use of the dataset and attach updated policies and guidelines if needed. </p> <ul> <li>Independent Review:</li> </ul> <p>Just like peer-reviewed journal papers, consider implementing an ethics review process, to evaluate the dataset creation process. Additionally, ensure you have applied for the requisite ethical approvals before collecting data e.g. IRB</p> <ul> <li>Fairness and Representation:</li> </ul> <p>Strive to create datasets that represent the diversity of the population, including gender, ethnicity, age, and other sociodemographic factors. Identify and mitigate or acknowledge biases in data, to ensure that the dataset does not perpetuate or amplify societal biases but is respectful of cultural norms.</p> <ul> <li>Resource Efficiency: </li> </ul> <p>Consider the environmental impact of dataset creation, such as the carbon footprint of large-scale data processing. Where possible record the computational resources used, how long it took to complete tasks and any other efficiency-saving techniques you used.</p> <ul> <li>Long-term Viability:</li> </ul> <p>Ensure that the dataset creation process is sustainable and does not deplete or degrade resources. It is important to question whether you need to go back to the same communities to use their data and how you ensure their participation is compensated.</p>"},{"location":"fair/","title":"FAIR Data Principles","text":""},{"location":"fair/#what-is-fair","title":"What is FAIR?","text":"<p>As more datasets for research and innovation are published openly, there is a need to have well-described, accessible data that conforms to community standards in order to discover and reuse relevant data, perform machine analysis at scale, or employ techniques such as artificial intelligence to identify patterns and correlations not visible to the human eye. </p> <p>The FAIR principles define the characteristics that data must have to be reused by humans and machines<sup>1</sup>. The FAIR data principles provide a set of guidelines to help make research data F-indable, A-ccessible, I-nteroperable, and R-eusable. These principles ensure that research objects are reusable and will indeed be reused, and thus become as valuable as possible. We base this toolkit on these fundamental principles of sustainable datasets. </p> Findable Ensuring that data is easily discoverable, identifiable and citeable.     Accessible Providing access to data in a format that is easy to download, use, and reuse i.e the technical means for accessing the data is available for widespread use.     Interoperable Ensuring that data can be integrated and combined with other data sources i.e. that data attributes across various datasets are consistent and understandable.     Reusable Ensuring that data can be reused for future research and analysis e.g. through clear documentation  and licensing frameworks     <ol> <li> <p>European Commission, Directorate-General for Research and Innovation, (2018). Turning FAIR into reality : final report and action plan from the European Commission expert group on FAIR data, Publications Office. https://data.europa.eu/doi/10.2777/1524 \u21a9</p> </li> </ol>"},{"location":"funding/","title":"Funding for Dataset Creation Projects","text":""},{"location":"funding/#tips-for-long-term-sustainability","title":"Tips for long-term sustainability","text":"<p>Like all projects,  the financial and human resources required to collect and create training data in low-resourced contexts can be substantial. Securing sustainable resourcing is important for the long-term accessibility and sustainability of training datasets. Resources may cover the cost of updating and maintaining datasets, organizing competitions, facilitating community engagement and advocacy, covering travel expenses for meetings or conferences, and other operational costs.  </p> <p>The following strategies can help you find financial and in-kind resources to sustain your training datasets and keep them relevant and viable in the long term:</p> <ol> <li>Grants: Securing funding through grants specifically aimed at covering the costs of updating the datasets, use-case development and incentives for contributors. This includes grants from governmental bodies, international organizations, educational institutions, and private foundations.</li> <li>Building Partnerships: Establishing partnerships with academic institutions, non-profits, and other organizations that would benefit from the datasets can create collaborations that provide additional expertise and contributions to the expansion and maintenance of datasets.</li> <li>Community Involvement in Fundraising: This can include crowdfunding campaigns where the community of researchers and beneficiaries directly contributes to the project\u2019s financial resources.</li> </ol>"},{"location":"hosting/","title":"Hosting your Datasets","text":"<p>Creating accessible datasets is an essential aspect of ensuring the sustainability of AI training datasets. </p>"},{"location":"hosting/#1-publish-in-widely-acceptable-and-available-data-sharing-platforms","title":"1. Publish in widely acceptable and available data-sharing\u00a0platforms","text":"<p>Researchers can promote accessibility by archiving and sharing the training data through widely accepted and available data repositories. Per Lacuna Fund requirements, these repositories should be public and open-access. </p> <p>Archiving data through these platforms ensures that the data is preserved for the long term, enabling other researchers and stakeholders to access and reuse the data for future research and applications. Additionally, these platforms provide a stable, persistent and reliable environment for data storage. The choice of hosting on widely accepted platforms makes it easier for researchers to discover and access relevant datasets, promoting the reuse and impact of your dataset.</p> <p>Most machine-learning communities have established go-to repositories to find training datasets for their models. Additionally, the choice of platform depends on your intended audience and the use cases for your dataset.</p> <p>Refer to the Hosting Guidelines for other considerations when selecting a publishing platform.</p> <p>Example</p> <p>Some hosting platforms to consider: </p> <p>Havard Dataverse</p> <p>Kaggle </p> <p>Github </p> <p>Zenodo </p>"},{"location":"hosting/#2-license-the-data-with-appropriate-open-source-licenses","title":"2. License the data with appropriate open-source licenses","text":"<p>Licensing AI training datasets with appropriate open-source licenses is critical for promoting the accessibility and reuse of the data. While accessibility in the context of FAIR does not necessarily mean free access for all, the Lacuna Fund grant agreement provides additional guidelines on which appropriate open-source licensing to use including Apache 2.0 for any code or other inventions, CC-BY 4.0 International, or CC BY-SA 4.0 for any other intellectual property (e.g., creative works that are not code or patentable). Open-source licenses provide a legal framework for sharing and using data, enabling others to access and build on your training datasets in accordance with the prescribed requirements attached to the license.  </p> <p>Example</p> <p>Some commonly used licenses included Creative Commons Licenses  and MIT licenses.</p> <p>The Creative Commons license is a widely used license that provides a flexible and standardized way for dataset owners to license data, allowing others to use the data while respecting the original creator's rights. Most publishing platforms automatically apply a license to the dataset during the publication process.</p>"},{"location":"hosting/#3-compress-the-data","title":"3. Compress the data","text":"<p>Compressing your AI training data is an effective way to reduce the storage space and bandwidth required to transfer large datasets. It also provides researchers with an opportunity to minimize the cost of access and the environmental impact of their datasets by reducing energy consumption during the processing and storage of data. There are several techniques that researchers can use to compress data, including lossless compression and lossy compression.</p> <p>Lossless compression techniques enable data to be compressed without any loss of information. This means that the original data can be fully reconstructed from the compressed data. Common lossless compression techniques include the gzip and zip formats, which are widely used for compressing text-based data such as CSV or TXT files. These formats can reduce the size of data by up to 70% without any loss of information.</p> <p>Lossy compression techniques enable data to be compressed by sacrificing some level of detail or accuracy. These techniques are commonly used for compressing image or audio data, where small losses in quality are often imperceptible. Common lossy compression techniques include JPEG for image data and MP3 for audio data. These formats can reduce the size of data by up to 90% or more, but at the cost of some loss of detail or accuracy.</p> <p>Data compression for  image datasets is usually required by some deep learning models and this compression can be done before publishing the datasets. </p>"},{"location":"privacy/","title":"Create Privacy-Preserving Datasets","text":"<p>Privacy-preserving techniques are essential when publishing training datasets in open formats, as they help protect the sensitive information of individuals in the dataset. Several techniques can be employed to ensure privacy while maintaining the utility of the data for research and development purposes as shown in the examples below.</p> NLPAgricultureHealth <p>The technique employed is highly dependent on the NLP task intended for the datasets\u00a0e.g. carrying out Name Entity Recognition (NER) tasks to categorise the entities appropriately then can eliminate Nouns such as Personally-Identifying Information as needed. It might be helpful to consider leaving the tokenized entities in the datasets\u00a0for various use cases.</p> <p>Other techniques useful for NLP dataset anonymization include:</p> <ul> <li>Noise Addition: Randomly swap words/characters with plausible alternatives using rules or language models. Maintains general statistics while obfuscating details.</li> <li>K-Anonymity Filtering: Keep sentences that occur a minimum number of times k reducing the chance of identifying rare sentences.</li> <li>Differential Privacy: Allows the performance of aggregate analytics over textual data while preserving individual privacy and works by adding calculated noise to aggregated statistics like word frequencies, co-occurrence counts etc.</li> <li>Synthetic Data Generation: This technique involves using algorithms to generate new textual data that can mimic human language patterns.</li> </ul> <p>In the case of geospatial and earth observation data, you can consider privacy-preserving mechanisms for AI training datasets that contain sensitive information e.g. farm boundaries including:</p> <ul> <li>Generalization: Aggreate and replace individual farm coordinates with regions (e.g.use of administrative zones such as adm3 typically used for wards/divisions).</li> <li>Geospatial obfuscation and differential privacy: Include random/controlleed noise to location data while maintaining statistical properties or use bounding boxes instead of precise farm polygons.</li> </ul> <p>Anonymising health data is a delicate balance between protecting patient privacy and retaining the data's utility for research. It's important to regularly review and update anonymization practices to align with evolving technology, regulations, and ethical standards. Multiple peer reviewers can quality check the final dataset contents and metadata before publication. Some approaches to consider include:</p> <ul> <li>Data anonymization: Remove personally identifiable information (PII) from the dataset, such as names, addresses, phone numbers, and email addresses. This process reduces the risk of re-identification of individuals.</li> <li>Pseudonymization: Replace identifiers with pseudonyms or codes that can be used to re-identify the data if necessary, but only with access a the key.</li> <li>**Data aggregation: Combine individual data points into aggregated groups, summarizing the data while reducing the risk of identifying individual entries. For example, instead of sharing exact ages, group them into age ranges.</li> <li>Data Perturbation: Modify data slightly e.g. altering dates (such as birth dates, admission, and discharge dates) by a random amount to prevent identification but maintaining the relative sequence of events to prevent identification while preserving the overall utility of the dataset.</li> <li>Masking: For images that capture facial features blurring, pixelation, or masking can be used to obscure the face.</li> <li>Redaction:Manually or automatically redact text e.g. in clinical notes or image annotations that might contain identifiable information.</li> </ul> <p>It is important when creating datasets for sharing and publishing to follow all relevant laws and regulations, such as GDPR and HIPAA for data protection and privacy. Many countries also have localized  data protection regulations that can be referenced before publication. </p> <p>Where these laws are lacking or limited, you can also create a code of ethics/data governance policy proscribing irresponsible and unethical use of the datasets, especially where no specific law or data protection legal structure exists to be adhered to.    </p>"}]}